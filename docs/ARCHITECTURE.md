# GridKV Architecture

**Version**: v3.1  
**Type**: System Design

---

## ğŸ—ï¸ Overview

GridKV is an **embedded distributed key-value cache** built on proven distributed systems principles:
- Consistent hashing (Dynamo)
- Gossip protocol (SWIM)
- Quorum replication
- Hybrid Logical Clock (HLC)

---

## ğŸ“ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             GridKV Instance (Embedded in App)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    Public API                           â”‚ â”‚
â”‚  â”‚  Set(key, value) â”‚ Get(key) â”‚ Delete(key)             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                     â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              Gossip Manager                             â”‚ â”‚
â”‚  â”‚  â€¢ Cluster membership (SWIM)                           â”‚ â”‚
â”‚  â”‚  â€¢ Failure detection (<1s)                             â”‚ â”‚
â”‚  â”‚  â€¢ Quorum replication (N/W/R)                          â”‚ â”‚
â”‚  â”‚  â€¢ Data synchronization                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                    â”‚                  â”‚            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Consistent    â”‚  â”‚  Storage        â”‚  â”‚  Network     â”‚  â”‚
â”‚  â”‚ Hash Ring     â”‚  â”‚  Backend        â”‚  â”‚  Transport   â”‚  â”‚
â”‚  â”‚               â”‚  â”‚                 â”‚  â”‚              â”‚  â”‚
â”‚  â”‚ â€¢ 150 vnodes  â”‚  â”‚ â€¢ MemSharded    â”‚  â”‚ â€¢ TCP        â”‚  â”‚
â”‚  â”‚ â€¢ O(log n)    â”‚  â”‚ â€¢ 32-64 shards  â”‚  â”‚ â€¢ Gossip     â”‚  â”‚
â”‚  â”‚   lookup      â”‚  â”‚ â€¢ 1M+ ops/s     â”‚  â”‚ â€¢ Adaptive   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Core Components

### 1. Consistent Hash Ring

**Purpose**: Distribute keys evenly across instances

**Implementation**: Dynamo-style with virtual nodes

**Algorithm**:
```
1. Hash each instance R times (R = 150 virtual nodes)
2. Place virtual nodes on hash ring (0 to 2^32-1)
3. For a key: hash(key) â†’ find next node clockwise
4. For replication: find N consecutive unique nodes
```

**Properties**:
- Load balance: Virtual nodes improve uniformity
- Minimal disruption: Only 1/M keys move when M nodes change
- Deterministic: Same key always maps to same nodes

**Performance**:
- Lookup: O(log n) binary search
- Add node: O(R + N) sorted merge
- Remove node: O(R + N) filtered scan

See: [CONSISTENT_HASHING paper](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf)

### 2. Gossip Protocol (SWIM)

**Purpose**: Cluster membership and failure detection

**Components**:
- **Membership**: Track which instances are alive/suspect/dead
- **Failure Detection**: Probe instances, detect failures in <1s
- **Dissemination**: Spread membership updates via epidemic broadcast

**How it Works**:
```
Every 1 second (GossipInterval):
  1. Select K random peers (fanout)
  2. Send membership state
  3. Receive their state
  4. Merge states (newest wins)
  5. Mark unresponsive instances as suspect
  6. Mark long-suspect instances as dead
```

**Adaptive**: Adjusts interval based on network latency (LAN vs WAN)

See: [GOSSIP_PROTOCOL.md](GOSSIP_PROTOCOL.md)

### 3. Storage Backend

**Purpose**: Local key-value storage on each instance

**Implementations**:
- **Memory**: Simple sync.Map (dev/test)
- **MemorySharded**: 32-64 sharded maps (production)

**Features**:
- Thread-safe concurrent access
- Deep-copy semantics (returned data safe to modify)
- TTL support (optional expiration)
- Sync buffer for Gossip replication

**Performance**:
- MemorySharded: 1-2M+ ops/s (recommended)
- Memory: 600-700K ops/s

See: [STORAGE_BACKENDS.md](STORAGE_BACKENDS.md)

### 4. Network Transport

**Purpose**: Inter-instance communication

**Protocol**: TCP (reliable, ordered delivery)

**Features**:
- Connection pooling (reuse connections)
- Adaptive timeouts (based on RTT)
- Auto-reconnect on failure

See: [TRANSPORT_LAYER.md](TRANSPORT_LAYER.md)

### 5. Hybrid Logical Clock (HLC)

**Purpose**: Distributed timestamps for conflict resolution

**Properties**:
- Causality: A â†’ B implies HLC(A) < HLC(B)
- Bounded drift: Stays within Îµ of physical time
- Monotonic: Never decreases

**Usage**: Version numbers for last-write-wins

See: [HYBRID_LOGICAL_CLOCK.md](HYBRID_LOGICAL_CLOCK.md)

---

## ğŸ”€ Data Flow

### Write Operation (Set)

```
Application calls kv.Set(ctx, "user:123", data)
  â†“
1. Consistent Hash: Find N instances for "user:123"
   â†’ ["instance-2", "instance-1", "instance-3"]
  â†“
2. Generate HLC timestamp (version)
  â†“
3. Write to N instances in parallel
  â†“
4. Wait for W confirmations (quorum)
  â†“
5. Return success to application
  â†“
6. Continue async replication to remaining instances
```

**Latency**: ~2ms for W=2 (LAN)

### Read Operation (Get)

```
Application calls kv.Get(ctx, "user:123")
  â†“
1. Consistent Hash: Find N instances for "user:123"
   â†’ ["instance-2", "instance-1", "instance-3"]
  â†“
2. Check if local instance is in N
   â†’ Yes: Read locally (43ns, in-process) âœ…
   â†’ No: Read from R remote instances
  â†“
3. If remote: Read from R instances in parallel
  â†“
4. Return value with highest version (newest)
  â†“
5. If versions differ: Trigger read-repair (async)
```

**Latency**: 43ns (local) or ~1ms (remote, LAN)

### Failure Detection

```
Every 1 second:
  â†“
1. Select random instance to probe
  â†“
2. Send ping
  â†“
3a. Response received â†’ Mark as alive âœ…
3b. No response â†’ Mark as suspect âš ï¸
  â†“
4. If suspect > SuspectTimeout (10s) â†’ Mark as dead âŒ
  â†“
5. Broadcast state change via Gossip
  â†“
6. Other instances re-route traffic
```

---

## ğŸŒ Multi-Datacenter Support

GridKV automatically detects and optimizes for multi-DC deployments:

```
Instance A (US-East) â†â†’ Instance B (US-West):
  Measure RTT: 20ms â†’ Classify as LAN
  â†’ Fast Gossip interval (1s)
  â†’ Sync reads preferred

Instance A (US-East) â†â†’ Instance C (EU-West):
  Measure RTT: 150ms â†’ Classify as WAN
  â†’ Slow Gossip interval (4s)
  â†’ Async replication
  â†’ Nearest DC reads
```

**Automatic Adaptation**:
- âœ… RTT measurement between all instances
- âœ… Dynamic Gossip interval adjustment
- âœ… Locality-aware read routing
- âœ… Cross-DC async replication

---

## ğŸ” Security

### Message Signing (Optional)

```go
GridKVOptions{
    EnableCrypto: true,  // Enable Ed25519 signatures
}
```

**When enabled**:
- All Gossip messages signed with Ed25519
- Prevents message tampering
- Authenticates sender identity
- ~15Âµs overhead per message

**When to use**:
- Enable: Untrusted networks, public cloud
- Disable: Private networks, trusted LANs

---

## ğŸ“ˆ Scalability

### Horizontal Scaling

```
1 instance:   1-2M ops/s
3 instances:  3-6M ops/s (linear)
10 instances: 10-20M ops/s (linear)
```

**Scales linearly** because:
- Data partitioned via consistent hashing
- Each instance handles its partition
- No central bottleneck

### Cluster Size Limits

| Cluster Size | Gossip Overhead | Use Case |
|--------------|----------------|----------|
| 1-10 instances | Negligible | Small deployments |
| 10-50 instances | Low (~1% network) | Medium deployments |
| 50-100 instances | Moderate (~2-3%) | Large deployments |
| 100+ instances | Higher | Consider hierarchical |

**Recommended**: 3-20 instances per datacenter

---

## ğŸ¯ Design Principles

### 1. Simplicity Over Features

**GridKV focuses on**:
- âœ… Simple KV operations (Set, Get, Delete)
- âœ… Automatic clustering
- âœ… Embedded deployment

**GridKV does NOT provide**:
- âŒ Rich data structures (List, Set, ZSet)
- âŒ Complex queries
- âŒ Lua scripting

**Philosophy**: Do one thing well (distributed KV cache)

### 2. Operational Simplicity

- âœ… Zero external dependencies
- âœ… Auto-clustering (no manual setup)
- âœ… Self-healing (automatic failover)
- âœ… One system to manage (not two)

### 3. Go-Native Integration

- âœ… Import as Go library
- âœ… Type-safe APIs
- âœ… Compile into single binary
- âœ… No FFI/RPC overhead

---

## ğŸ“š Related Documentation

- [Embedded Architecture](EMBEDDED_ARCHITECTURE.md) - Why embedded?
- [Consistency Model](CONSISTENCY_MODEL.md) - Quorum details
- [Gossip Protocol](GOSSIP_PROTOCOL.md) - SWIM specification
- [Performance](PERFORMANCE.md) - Benchmarks

---

**GridKV Architecture** - Embedded, Distributed, Simple âœ…

**Last Updated**: 2025-11-09  
**GridKV Version**: v3.1
